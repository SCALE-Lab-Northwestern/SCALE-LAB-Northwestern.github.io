- title: "Scaling Scientific Machine Learning: Integrating Theory and Numerics in Both Training and Inference"
  authors:  Yiping Lu
  cv: https://2prime.github.io/
  abstract:  "Scaling scientific machine learning (SciML) requires overcoming bottlenecks at both training and inference. On the training side, we study the statistical convergence rate and limits of deep learning for solving elliptic PDEs from random samples. While our theory predicts optimal polynomial convergence for PINNs, optimization becomes prohibitively ill-conditioned as networks widen. By adapting descent strategies to the optimization geometry, we obtain scale-invariant training dynamics that translate polynomial convergence into concrete compute and yield compute-optimal configurations. On the inference side, I will introduce Simulation-Calibrated SciML (SCaSML), a physics-informed post-processing framework that improves surrogate models without retraining or fine-tuning. By enforcing physical laws, SCaSML delivers trustworthy corrections (via Feynman-Kac simulation) with approximate confidence intervals, achieves faster and near-optimal convergence rates, and supports online updates for digital twins. Together, these results integrate theory and numerics to enable predictable, reliable scaling of SciML in both training and inference. This is based on joint work with Lexing Ying, Jose Blanchet, Haoxuan Chen, Zexi Fan, Youheng Zhu, Shihao Yang, Jasen Lai, Sifan Wang, and Chunmei Wang."
  reference: "https://arxiv.org/abs/2504.16172"
