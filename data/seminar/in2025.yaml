- title: "Probabilistic Foundations of Diffusion Models and Monte Carlo PDE Solvers: From Particle Systems to Macroscopic Dynamics"
  authors: Yiping Lu
  cv: https://2prime.github.io/
  abstract: "We study large systems of interacting stochastic particles undergoing motion, birth, and death, and explore the probabilistic mechanisms connecting microscopic randomness to macroscopic deterministic behavior. Each particle follows a stochastic differential equation with position-dependent drift and diffusion and experiences independent birth-death events modeled by exponential clocks. The state of the system is described through its empirical measure, for which we establish a martingale decomposition of test function observables. As the number of particles tends to infinity, the empirical measure converges in probability to a deterministic limit, recovering classical advection-diffusion-reaction PDEs as a law of large numbers. This probabilistic framework forms the basic foundation for modern diffusion models and Monte Carlo PDE solvers. We further illustrate how forward and backward stochastic differential equations can be interpreted to simulate, denoise, and reconstruct distributions, and discuss Feynmanâ€“Kac representations and particle methods, emphasizing their convergence and stochastic underpinnings."
  reference: 
   - text: lecture note 
     url: https://www.overleaf.com/read/hhqjcmfffvnt#4154a4
   - text: note
     url: https://drive.google.com/file/d/1pf4QYqf3r4QxEku5w1KdH2XBeGC0pz7M/view?usp=sharing
   - text: diffusion model paper
     url: https://arxiv.org/abs/2011.13456
  date: "10/3/2025,10/17/2025"

- title: "Mordern Optimizers For LLMs"
  authors: Yiping Lu
  cv: https://2prime.github.io/
  abstract: "Optimization lies at the heart of modern machine learning and numerical computation. In this seminar, we provide a comprehensive overview of recent developments in optimization algorithms. We begin with classical and quasi-Newton methods, including BFGS, L-BFGS, and K-FAC, highlighting their efficiency in approximating second-order information. Next, we explore adaptive algorithms inspired by Adagrad, such as Adam, Shampoo, and SOAP, which leverage per-parameter scaling to accelerate convergence. Finally, we discuss geometry-aware methods, including SignGD and MUON, which exploit alternative geometrical structures to improve optimization performance. The seminar aims to give participants both a conceptual understanding and practical insights into these cutting-edge algorithms, bridging theoretical ideas with real-world applications."
  reference:
 - text: Steepest Descent
    url: https://arxiv.org/abs/2409.20325
  - text: MUON
    url: https://kellerjordan.github.io/posts/muon/
  - text: Kron
    url:  https://arxiv.org/pdf/1512.04202 
  - text: noisy natural gradient as variational inference
    url: https://arxiv.org/abs/1712.02390
  - text: KL-SOAP
    url: https://arxiv.org/abs/2509.03378
  - text: SOAP for PINN
    url: https://arxiv.org/html/2502.00604v1 
  - text: SOAP 
     url: https://arxiv.org/abs/2409.11321
  - text: K-FAC
     url: https://arxiv.org/abs/1503.05671
  - text: muP scaling
     url: https://arxiv.org/abs/2203.03466
  - text: Benchmarking Paper 1
    url: https://arxiv.org/abs/2509.02046
  - text: Benchmarking Paper 2
    url: https://arxiv.org/abs/2509.01440 
  date: "11/14/2025"


- title: "Discrete Diffusion Model"
  authors: Jinhua Lyu
  abstract: " "
  date: "TBD"
  
- title: "Preodictin Powered Inference"
  authors: TBS
  abstract: " "
  date: "TBD"
  

  
#- title: "Scaling Scientific Machine Learning: Integrating Theory and Numerics in Both Training and Inference"
#  authors:  Yiping Lu
#  cv: https://2prime.github.io/
#  abstract:  "Scaling scientific machine learning (SciML) requires overcoming bottlenecks at both training and inference. On the training side, we study the statistical convergence rate and limits of deep learning for solving elliptic PDEs from random samples. While our theory predicts optimal polynomial convergence for PINNs, optimization becomes prohibitively ill-conditioned as networks widen. By adapting descent strategies to the optimization geometry, we obtain scale-invariant training dynamics that translate polynomial convergence into concrete compute and yield compute-optimal configurations. On the inference side, I will introduce Simulation-Calibrated SciML (SCaSML), a physics-informed post-processing framework that improves surrogate models without retraining or fine-tuning. By enforcing physical laws, SCaSML delivers trustworthy corrections (via Feynman-Kac simulation) with approximate confidence intervals, achieves faster and near-optimal convergence rates, and supports online updates for digital twins. Together, these results integrate theory and numerics to enable predictable, reliable scaling of SciML in both training and inference. This is based on joint work with Lexing Ying, Jose Blanchet, Haoxuan Chen, Zexi Fan, Youheng Zhu, Shihao Yang, Jasen Lai, Sifan Wang, and Chunmei Wang."
#  reference: 
#    - text: Neural Scaling Law
#      url: https://arxiv.org/abs/2102.06701
#    - text: chinchilla scaling laws
#      url: https://arxiv.org/abs/2203.15556
#    - text: Inference Time Scaling
#      url: https://arxiv.org/abs/2408.00724
#    - text: Inference Time Scaling for PDE
#      url: https://arxiv.org/abs/2504.16172
#  date: "TBD"
