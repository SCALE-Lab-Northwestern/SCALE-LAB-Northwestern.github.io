<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Posts - SCALE Laboratory @ Northwestern</title><link>https://scale-lab-northwestern.github.io/posts/</link><description>Posts | SCALE Laboratory @ Northwestern</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Thu, 12 May 2022 09:40:28 +0800</lastBuildDate><atom:link href="https://scale-lab-northwestern.github.io/posts/" rel="self" type="application/rss+xml"/><item><title>Scaling Law of Scientific Machine Learning</title><link>https://scale-lab-northwestern.github.io/posts/scalinglaw/</link><pubDate>Thu, 12 May 2022 09:40:28 +0800</pubDate><author>Author</author><guid>https://scale-lab-northwestern.github.io/posts/scalinglaw/</guid><description><![CDATA[SCALE Lab&rsquo;s publicatoin in this direction Note <i class="far fa-bookmark fa-fw"></i>&nbsp;SCALE lab is always actively building scaling law for scientific machine learning, here&rsquo;s the achievement of lab members on this direction
Yiping Lu, Haoxuan Chen, Jianfeng Lu, Lexing Ying, Jose Blanchet Machine Learning For Elliptic PDEs: Fast Rate Generalization Bound, Neural Scaling Law and Minimax Optimality, Tenth International Conference on Learning Representations(ICLR) 2022 Yiping Lu, Jose Blanchet,Lexing Ying Sobolev Acceleration and Statistical Optimality for Learning Elliptic Equations via Gradient Descent, Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS) 2022 Jikai Jin, Yiping Lu, Jose Blanchet, Lexing Ying Minimax Optimal Kernel Operator Learning via Multilevel Training, Eleventh International Conference on Learning Representations(ICLR) 2023 Honam Wong, Wendao Wu, Fanghui Liu, Yiping Lu Physics-Informed Learning Interpolates Well in Fixed Dimension: Inductive Bias and Benign Overfitting, Submitted Scaling Law The term “scaling laws” in deep learning refers to relations between functional properties of interest (usually the test loss or some performance metric for fine-tuning tasks) and properties of the architecture or optimization process (like model size, width, or training compute).]]></description></item><item><title>Simulation Calibrated Scientific Machine Learning</title><link>https://scale-lab-northwestern.github.io/posts/scasml/</link><pubDate>Thu, 12 May 2022 09:40:28 +0800</pubDate><author>Author</author><guid>https://scale-lab-northwestern.github.io/posts/scasml/</guid><description>TO DO</description></item></channel></rss>