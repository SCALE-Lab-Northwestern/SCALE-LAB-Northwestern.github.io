<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Posts - SCALE Laboratory @ Northwestern</title><link>https://scale-lab-northwestern.github.io/posts/</link><description>Posts | SCALE Laboratory @ Northwestern</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Thu, 12 May 2022 09:40:28 +0800</lastBuildDate><atom:link href="https://scale-lab-northwestern.github.io/posts/" rel="self" type="application/rss+xml"/><item><title>Scaling Law of Scientific Machine Learning</title><link>https://scale-lab-northwestern.github.io/posts/scalinglaw/</link><pubDate>Thu, 12 May 2022 09:40:28 +0800</pubDate><author>Author</author><guid>https://scale-lab-northwestern.github.io/posts/scalinglaw/</guid><description>Scaling Law The term “scaling laws” in deep learning refers to relations between functional properties of interest (usually the test loss or some performance metric for fine-tuning tasks) and properties of the architecture or optimization process (like model size, width, or training compute). These laws can help inform the design and training of deep learning models, as well as provide insights into their underlying principles.</description></item><item><title>Simulation Calibrated Scientific Machine Learning</title><link>https://scale-lab-northwestern.github.io/posts/scasml/</link><pubDate>Thu, 12 May 2022 09:40:28 +0800</pubDate><author>Author</author><guid>https://scale-lab-northwestern.github.io/posts/scasml/</guid><description>TO DO</description></item></channel></rss>