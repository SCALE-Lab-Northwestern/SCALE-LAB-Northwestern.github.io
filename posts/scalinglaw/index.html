<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><meta http-equiv=X-UA-Compatible content="IE=edge, chrome=1"><title>Scaling Law of Scientific Machine Learning - SCALE Laboratory @ Northwestern</title><meta name=Description content="The website of SCALE Lab @ Northwestern "><meta property="og:url" content="https://scale-lab-northwestern.github.io/posts/scalinglaw/">
<meta property="og:site_name" content="SCALE Laboratory @ Northwestern"><meta property="og:title" content="Scaling Law of Scientific Machine Learning"><meta property="og:description" content="SCALE Lab’s publicatoin in this direction Note :(far fa-bookmark fa-fw): SCALE lab is always actively building scaling law for scientific machine learning, here’s the achievement of lab members on this direction
Yiping Lu, Haoxuan Chen, Jianfeng Lu, Lexing Ying, Jose Blanchet Machine Learning For Elliptic PDEs: Fast Rate Generalization Bound, Neural Scaling Law and Minimax Optimality, Tenth International Conference on Learning Representations(ICLR) 2022 Yiping Lu, Jose Blanchet,Lexing Ying Sobolev Acceleration and Statistical Optimality for Learning Elliptic Equations via Gradient Descent, Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS) 2022 Jikai Jin, Yiping Lu, Jose Blanchet, Lexing Ying Minimax Optimal Kernel Operator Learning via Multilevel Training, Eleventh International Conference on Learning Representations(ICLR) 2023 Honam Wong, Wendao Wu, Fanghui Liu, Yiping Lu Physics-Informed Learning Interpolates Well in Fixed Dimension: Inductive Bias and Benign Overfitting, Submitted Scaling Law The term “scaling laws” in deep learning refers to relations between functional properties of interest (usually the test loss or some performance metric for fine-tuning tasks) and properties of the architecture or optimization process (like model size, width, or training compute)."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-05-12T09:40:28+08:00"><meta property="article:modified_time" content="2022-05-12T09:40:28+08:00"><meta property="article:tag" content="Scientific Machine Learning"><meta property="og:image" content="https://scale-lab-northwestern.github.io/logo.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://scale-lab-northwestern.github.io/logo.png"><meta name=twitter:title content="Scaling Law of Scientific Machine Learning"><meta name=twitter:description content="SCALE Lab’s publicatoin in this direction Note :(far fa-bookmark fa-fw): SCALE lab is always actively building scaling law for scientific machine learning, here’s the achievement of lab members on this direction
Yiping Lu, Haoxuan Chen, Jianfeng Lu, Lexing Ying, Jose Blanchet Machine Learning For Elliptic PDEs: Fast Rate Generalization Bound, Neural Scaling Law and Minimax Optimality, Tenth International Conference on Learning Representations(ICLR) 2022 Yiping Lu, Jose Blanchet,Lexing Ying Sobolev Acceleration and Statistical Optimality for Learning Elliptic Equations via Gradient Descent, Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS) 2022 Jikai Jin, Yiping Lu, Jose Blanchet, Lexing Ying Minimax Optimal Kernel Operator Learning via Multilevel Training, Eleventh International Conference on Learning Representations(ICLR) 2023 Honam Wong, Wendao Wu, Fanghui Liu, Yiping Lu Physics-Informed Learning Interpolates Well in Fixed Dimension: Inductive Bias and Benign Overfitting, Submitted Scaling Law The term “scaling laws” in deep learning refers to relations between functional properties of interest (usually the test loss or some performance metric for fine-tuning tasks) and properties of the architecture or optimization process (like model size, width, or training compute)."><meta name=application-name content="SCALE Lab"><meta name=apple-mobile-web-app-title content="SCALE Lab"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel=stylesheet href=https://unpkg.com/swiper@8/swiper-bundle.min.css><link rel=icon href=/nu.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://scale-lab-northwestern.github.io/posts/scalinglaw/><link rel=prev href=https://scale-lab-northwestern.github.io/posts/scasml/><link rel=stylesheet href=/lib/normalize/normalize.min.css><link rel=stylesheet href=/css/style.min.css><link rel=stylesheet href=/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/lib/animate/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Scaling Law of Scientific Machine Learning","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/scale-lab-northwestern.github.io\/posts\/scalinglaw\/"},"genre":"posts","keywords":"Scientific Machine Learning","wordcount":247,"url":"https:\/\/scale-lab-northwestern.github.io\/posts\/scalinglaw\/","datePublished":"2022-05-12T09:40:28+08:00","dateModified":"2022-05-12T09:40:28+08:00","publisher":{"@type":"Organization","name":"Yiping"},"author":{"@type":"Person","name":"Yiping"},"description":""}</script></head><body header-desktop=fixed header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"light"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"light"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="SCALE Laboratory @ Northwestern"><span id=id-1 class=typeit></span></a></div><div class=menu><div class=menu-inner><a class=menu-item href=/>Home </a><a class=menu-item href=/news/>News </a><a class=menu-item href=/team/>Team </a><a class=menu-item href=/research/>Research </a><a class=menu-item href=/publications/>Publications </a><a class=menu-item href=/seminars/>Blogs </a><a class=menu-item href=/teaching/>Courses </a><a class=menu-item href=/contact/>Contact </a><span class="menu-item delimiter"></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw"></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="SCALE Laboratory @ Northwestern"><span id=id-2 class=typeit></span></a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/ title>Home</a><a class=menu-item href=/news/ title>News</a><a class=menu-item href=/team/ title>Team</a><a class=menu-item href=/research/ title>Research</a><a class=menu-item href=/publications/ title>Publications</a><a class=menu-item href=/seminars/ title>Blogs</a><a class=menu-item href=/teaching/ title>Courses</a><a class=menu-item href=/contact/ title>Contact</a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw"></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animated flipInX">Scaling Law of Scientific Machine Learning</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/ title=Author rel=author class=author><i class="fas fa-user-circle fa-fw"></i>Yiping</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2022-05-12>2022-05-12</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;247 words&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;2 minutes&nbsp;</div></div><div class="details toc" id=toc-static kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#scale-labs-publicatoin-in-this-direction>SCALE Lab&rsquo;s publicatoin in this direction</a></li><li><a href=#scaling-law>Scaling Law</a></li><li><a href=#statistical-model-for-physics-informed-machine-learning>Statistical Model For Physics Informed Machine Learning</a><ul><li><a href=#physics-informed-machine-learning>Physics-Informed Machine Learning</a></li><li><a href=#operator-learning>Operator Learning</a></li></ul></li><li><a href=#inductive-bias>Inductive Bias</a></li></ul></nav></div></div><div class=content id=content><h2 id=scale-labs-publicatoin-in-this-direction>SCALE Lab&rsquo;s publicatoin in this direction</h2><div class="details admonition note open"><div class="details-summary admonition-title"><i class="icon fas fa-pencil-alt fa-fw"></i>Note<i class="details-icon fas fa-angle-right fa-fw"></i></div><div class=details-content><div class=admonition-content><p><i class="far fa-bookmark fa-fw"></i>
SCALE lab is always actively building scaling law for scientific machine learning, here&rsquo;s the achievement of lab members on this direction</p><ul><li><em>Yiping Lu, Haoxuan Chen, Jianfeng Lu, Lexing Ying, Jose Blanchet</em> <strong>Machine Learning For Elliptic PDEs: Fast Rate Generalization Bound, Neural Scaling Law and Minimax Optimality</strong>, <em>Tenth International Conference on Learning Representations(ICLR) 2022</em></li><li><em>Yiping Lu, Jose Blanchet,Lexing Ying</em> <strong>Sobolev Acceleration and Statistical Optimality for Learning Elliptic Equations via Gradient Descent</strong>, <em>Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS) 2022</em></li><li><em>Jikai Jin, Yiping Lu, Jose Blanchet, Lexing Ying</em> <strong>Minimax Optimal Kernel Operator Learning via Multilevel Training</strong>, <em>Eleventh International Conference on Learning Representations(ICLR) 2023</em></li><li><em>Honam Wong, Wendao Wu, Fanghui Liu, Yiping Lu</em> <strong>Physics-Informed Learning Interpolates Well in Fixed Dimension: Inductive Bias and Benign Overfitting</strong>, <em>Submitted</em></li></ul></div></div></div><h2 id=scaling-law>Scaling Law</h2><p>The term “scaling laws” in deep learning refers to relations between functional properties of interest (usually the test loss or some performance metric for fine-tuning tasks) and properties of the architecture or optimization process (like model size, width, or training compute). These laws can help inform the design and training of deep learning models, as well as provide insights into their underlying principles.</p><p><img class=lazyload src=/svg/loading.min.svg data-src=./sclaing.png data-srcset="./sclaing.png, ./sclaing.png 1.5x, ./sclaing.png 2x" data-sizes=auto alt=./sclaing.png title=Research></p><h2 id=statistical-model-for-physics-informed-machine-learning>Statistical Model For Physics Informed Machine Learning</h2><h3 id=physics-informed-machine-learning>Physics-Informed Machine Learning</h3><p>Physics-Informed Machine Learning (PIML) aims to solve equation \(\mathcal{A}u=f\) using observations of function \(f\).</p><h3 id=operator-learning>Operator Learning</h3><h2 id=inductive-bias>Inductive Bias</h2><p>We also considered the following two estimators</p><ul><li><strong>Regularized Least Square</strong> \(\text{argmin}<em>u \|\mathcal{A}(u)-f\|^2\)\(+\|f\|</em>\beta^2\)</li><li><strong>Minimum Norm Interpolation</strong> \(\text{argmin}<em>u | |f|</em>\beta^2\) subject to \((\mathcal{A}u)(x_i)=f(x_i)\)</li></ul></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2022-05-12</span></div><div class=post-info-license></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://scale-lab-northwestern.github.io/posts/scalinglaw/ data-title="Scaling Law of Scientific Machine Learning" data-hashtags="Scientific Machine Learning"><i class="fab fa-twitter fa-fw"></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://scale-lab-northwestern.github.io/posts/scalinglaw/ data-hashtag="Scientific Machine Learning"><i class="fab fa-facebook-square fa-fw"></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://scale-lab-northwestern.github.io/posts/scalinglaw/ data-title="Scaling Law of Scientific Machine Learning"><i class="fab fa-hacker-news fa-fw"></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://scale-lab-northwestern.github.io/posts/scalinglaw/ data-title="Scaling Law of Scientific Machine Learning"><i data-svg-src=/lib/simple-icons/icons/line.min.svg></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://scale-lab-northwestern.github.io/posts/scalinglaw/ data-title="Scaling Law of Scientific Machine Learning"><i class="fab fa-weibo fa-fw"></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw"></i>&nbsp;<a href=/tags/scientific-machine-learning/>Scientific Machine Learning</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/posts/scasml/ class=prev rel=prev title="Simulation Calibrated Scientific Machine Learning"><i class="fas fa-angle-left fa-fw"></i>Simulation Calibrated Scientific Machine Learning</a></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2024</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/ target=_blank>SCALE Lab, Northwestern University.</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw"></i></a></div><link rel=stylesheet href=/lib/lightgallery/lightgallery.min.css><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=stylesheet href=/lib/katex/copy-tex.min.css><script type=text/javascript src=/lib/smooth-scroll/smooth-scroll.min.js></script><script type=text/javascript src=/lib/lazysizes/lazysizes.min.js></script><script type=text/javascript src=/lib/lightgallery/lightgallery.min.js></script><script type=text/javascript src=/lib/lightgallery/lg-thumbnail.min.js></script><script type=text/javascript src=/lib/lightgallery/lg-zoom.min.js></script><script type=text/javascript src=/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/lib/sharer/sharer.min.js></script><script type=text/javascript src=/lib/typeit/typeit.min.js></script><script type=text/javascript src=/lib/katex/katex.min.js></script><script type=text/javascript src=/lib/katex/auto-render.min.js></script><script type=text/javascript src=/lib/katex/copy-tex.min.js></script><script type=text/javascript src=/lib/katex/mhchem.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:10},comment:{},data:{"id-1":"SCALE Lab@ Northwestern","id-2":"SCALE Lab@ Northwestern"},lightGallery:{actualSize:!1,exThumbImage:"data-thumbnail",hideBarsDelay:2e3,selector:".lightgallery",speed:400,thumbContHeight:80,thumbWidth:80,thumbnail:!0},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},typeit:{cursorChar:null,cursorSpeed:null,data:{"id-1":["id-1"],"id-2":["id-2"]},duration:null,speed:null}}</script><script type=text/javascript src=/js/theme.min.js></script></body></html>